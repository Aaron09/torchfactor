{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchfactor.factorization.svdnet import SVDNet\n",
    "from torchfactor.experiment.experiment import Experiment\n",
    "from polyu_dataset import PolyUDataset\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_TYPE = \"mean\"\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VAL_BATCH_SIZE = 1\n",
    "\n",
    "training_dataset = PolyUDataset(split_type=\"train\", image_type=IMAGE_TYPE, downsample_shape=(128,128))\n",
    "training_dataloader = torch.utils.data.DataLoader(\n",
    "    training_dataset, batch_size=TRAIN_BATCH_SIZE, \n",
    "    shuffle=True, num_workers=0, drop_last=True\n",
    ")\n",
    "\n",
    "validation_dataset = PolyUDataset(split_type='val', image_type=IMAGE_TYPE, in_memory=False, downsample_shape=(128,128))\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=False, num_workers=0, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVDNet as the full network for factorization can only learn a single image\n",
    "indices = [np.random.randint(len(training_dataset))] # randomly select a single image\n",
    "\n",
    "single_ele_dataloader = torch.utils.data.DataLoader(\n",
    "    training_dataset, batch_size=TRAIN_BATCH_SIZE, \n",
    "    num_workers=0, drop_last=False,\n",
    "    sampler=torch.utils.data.SubsetRandomSampler(indices)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: total loss is 14044.435546875, avg loss is 14044.435546875\n",
      "epoch 0: val avg loss is 13878.0263671875\n",
      "epoch 1: total loss is 13878.0263671875, avg loss is 13878.0263671875\n",
      "epoch 1: val avg loss is 13786.6708984375\n",
      "epoch 2: total loss is 13786.6708984375, avg loss is 13786.6708984375\n",
      "epoch 2: val avg loss is 13707.44140625\n",
      "epoch 3: total loss is 13707.44140625, avg loss is 13707.44140625\n",
      "epoch 3: val avg loss is 13620.70703125\n",
      "epoch 4: total loss is 13620.70703125, avg loss is 13620.70703125\n",
      "epoch 4: val avg loss is 13564.42578125\n",
      "epoch 5: total loss is 13564.42578125, avg loss is 13564.42578125\n",
      "epoch 5: val avg loss is 13498.1572265625\n",
      "epoch 6: total loss is 13498.1572265625, avg loss is 13498.1572265625\n",
      "epoch 6: val avg loss is 13405.4990234375\n",
      "epoch 7: total loss is 13405.4990234375, avg loss is 13405.4990234375\n",
      "epoch 7: val avg loss is 13331.5419921875\n",
      "epoch 8: total loss is 13331.5419921875, avg loss is 13331.5419921875\n",
      "epoch 8: val avg loss is 13259.07421875\n",
      "epoch 9: total loss is 13259.07421875, avg loss is 13259.07421875\n",
      "epoch 9: val avg loss is 13171.85546875\n",
      "epoch 10: total loss is 13171.85546875, avg loss is 13171.85546875\n",
      "epoch 10: val avg loss is 13097.638671875\n",
      "epoch 11: total loss is 13097.638671875, avg loss is 13097.638671875\n",
      "epoch 11: val avg loss is 13016.8876953125\n",
      "epoch 12: total loss is 13016.8876953125, avg loss is 13016.8876953125\n",
      "epoch 12: val avg loss is 12918.9970703125\n",
      "epoch 13: total loss is 12918.9970703125, avg loss is 12918.9970703125\n",
      "epoch 13: val avg loss is 12834.4130859375\n",
      "epoch 14: total loss is 12834.4130859375, avg loss is 12834.4130859375\n",
      "epoch 14: val avg loss is 12738.0986328125\n",
      "epoch 15: total loss is 12738.0986328125, avg loss is 12738.0986328125\n",
      "epoch 15: val avg loss is 12646.93359375\n",
      "epoch 16: total loss is 12646.93359375, avg loss is 12646.93359375\n",
      "epoch 16: val avg loss is 12561.962890625\n",
      "epoch 17: total loss is 12561.962890625, avg loss is 12561.962890625\n",
      "epoch 17: val avg loss is 12477.5849609375\n",
      "epoch 18: total loss is 12477.5849609375, avg loss is 12477.5849609375\n",
      "epoch 18: val avg loss is 12376.453125\n",
      "epoch 19: total loss is 12376.453125, avg loss is 12376.453125\n",
      "epoch 19: val avg loss is 12265.810546875\n",
      "epoch 20: total loss is 12265.810546875, avg loss is 12265.810546875\n",
      "epoch 20: val avg loss is 12180.9931640625\n",
      "epoch 21: total loss is 12180.9931640625, avg loss is 12180.9931640625\n",
      "epoch 21: val avg loss is 12079.86328125\n",
      "epoch 22: total loss is 12079.86328125, avg loss is 12079.86328125\n",
      "epoch 22: val avg loss is 11992.1396484375\n",
      "epoch 23: total loss is 11992.1396484375, avg loss is 11992.1396484375\n",
      "epoch 23: val avg loss is 11922.3369140625\n",
      "epoch 24: total loss is 11922.3369140625, avg loss is 11922.3369140625\n",
      "epoch 24: val avg loss is 11867.5673828125\n",
      "epoch 25: total loss is 11867.5673828125, avg loss is 11867.5673828125\n",
      "epoch 25: val avg loss is 11794.4228515625\n",
      "epoch 26: total loss is 11794.4228515625, avg loss is 11794.4228515625\n",
      "epoch 26: val avg loss is 11663.6005859375\n",
      "epoch 27: total loss is 11663.6005859375, avg loss is 11663.6005859375\n",
      "epoch 27: val avg loss is 11544.1669921875\n",
      "epoch 28: total loss is 11544.1669921875, avg loss is 11544.1669921875\n",
      "epoch 28: val avg loss is 11477.03125\n",
      "epoch 29: total loss is 11477.03125, avg loss is 11477.03125\n",
      "epoch 29: val avg loss is 11366.0419921875\n",
      "epoch 30: total loss is 11366.0419921875, avg loss is 11366.0419921875\n",
      "epoch 30: val avg loss is 11282.2841796875\n",
      "epoch 31: total loss is 11282.2841796875, avg loss is 11282.2841796875\n",
      "epoch 31: val avg loss is 11199.7392578125\n",
      "epoch 32: total loss is 11199.7392578125, avg loss is 11199.7392578125\n",
      "epoch 32: val avg loss is 11105.400390625\n",
      "epoch 33: total loss is 11105.400390625, avg loss is 11105.400390625\n",
      "epoch 33: val avg loss is 11026.7861328125\n",
      "epoch 34: total loss is 11026.7861328125, avg loss is 11026.7861328125\n",
      "epoch 34: val avg loss is 10936.951171875\n",
      "epoch 35: total loss is 10936.951171875, avg loss is 10936.951171875\n",
      "epoch 35: val avg loss is 10853.0234375\n",
      "epoch 36: total loss is 10853.0234375, avg loss is 10853.0234375\n",
      "epoch 36: val avg loss is 10787.70703125\n",
      "epoch 37: total loss is 10787.70703125, avg loss is 10787.70703125\n",
      "epoch 37: val avg loss is 10744.2939453125\n",
      "epoch 38: total loss is 10744.2939453125, avg loss is 10744.2939453125\n",
      "epoch 38: val avg loss is 10683.9677734375\n",
      "epoch 39: total loss is 10683.9677734375, avg loss is 10683.9677734375\n",
      "epoch 39: val avg loss is 10553.712890625\n",
      "epoch 40: total loss is 10553.712890625, avg loss is 10553.712890625\n",
      "epoch 40: val avg loss is 10488.46484375\n",
      "epoch 41: total loss is 10488.46484375, avg loss is 10488.46484375\n",
      "epoch 41: val avg loss is 10360.0791015625\n",
      "epoch 42: total loss is 10360.0791015625, avg loss is 10360.0791015625\n",
      "epoch 42: val avg loss is 10300.0634765625\n",
      "epoch 43: total loss is 10300.0634765625, avg loss is 10300.0634765625\n",
      "epoch 43: val avg loss is 10204.4794921875\n",
      "epoch 44: total loss is 10204.4794921875, avg loss is 10204.4794921875\n",
      "epoch 44: val avg loss is 10140.9794921875\n",
      "epoch 45: total loss is 10140.9794921875, avg loss is 10140.9794921875\n",
      "epoch 45: val avg loss is 10055.501953125\n",
      "epoch 46: total loss is 10055.501953125, avg loss is 10055.501953125\n",
      "epoch 46: val avg loss is 10000.833984375\n",
      "epoch 47: total loss is 10000.833984375, avg loss is 10000.833984375\n",
      "epoch 47: val avg loss is 9964.3203125\n",
      "epoch 48: total loss is 9964.3203125, avg loss is 9964.3203125\n",
      "epoch 48: val avg loss is 9911.205078125\n",
      "epoch 49: total loss is 9911.205078125, avg loss is 9911.205078125\n",
      "epoch 49: val avg loss is 9874.2607421875\n",
      "epoch 50: total loss is 9874.2607421875, avg loss is 9874.2607421875\n",
      "epoch 50: val avg loss is 9790.3466796875\n",
      "epoch 51: total loss is 9790.3466796875, avg loss is 9790.3466796875\n",
      "epoch 51: val avg loss is 9666.2509765625\n",
      "epoch 52: total loss is 9666.2509765625, avg loss is 9666.2509765625\n",
      "epoch 52: val avg loss is 9588.349609375\n",
      "epoch 53: total loss is 9588.349609375, avg loss is 9588.349609375\n",
      "epoch 53: val avg loss is 9483.1416015625\n",
      "epoch 54: total loss is 9483.1416015625, avg loss is 9483.1416015625\n",
      "epoch 54: val avg loss is 9425.30859375\n",
      "epoch 55: total loss is 9425.30859375, avg loss is 9425.30859375\n",
      "epoch 55: val avg loss is 9327.7470703125\n",
      "epoch 56: total loss is 9327.7470703125, avg loss is 9327.7470703125\n",
      "epoch 56: val avg loss is 9266.908203125\n",
      "epoch 57: total loss is 9266.908203125, avg loss is 9266.908203125\n",
      "epoch 57: val avg loss is 9187.58203125\n",
      "epoch 58: total loss is 9187.58203125, avg loss is 9187.58203125\n",
      "epoch 58: val avg loss is 9119.41015625\n",
      "epoch 59: total loss is 9119.41015625, avg loss is 9119.41015625\n",
      "epoch 59: val avg loss is 9060.001953125\n",
      "epoch 60: total loss is 9060.001953125, avg loss is 9060.001953125\n",
      "epoch 60: val avg loss is 9000.81640625\n",
      "epoch 61: total loss is 9000.81640625, avg loss is 9000.81640625\n",
      "epoch 61: val avg loss is 8934.83203125\n",
      "epoch 62: total loss is 8934.83203125, avg loss is 8934.83203125\n",
      "epoch 62: val avg loss is 8838.958984375\n",
      "epoch 63: total loss is 8838.958984375, avg loss is 8838.958984375\n",
      "epoch 63: val avg loss is 8766.3828125\n",
      "epoch 64: total loss is 8766.3828125, avg loss is 8766.3828125\n",
      "epoch 64: val avg loss is 8707.986328125\n",
      "epoch 65: total loss is 8707.986328125, avg loss is 8707.986328125\n",
      "epoch 65: val avg loss is 8642.6962890625\n",
      "epoch 66: total loss is 8642.6962890625, avg loss is 8642.6962890625\n",
      "epoch 66: val avg loss is 8575.1796875\n",
      "epoch 67: total loss is 8575.1796875, avg loss is 8575.1796875\n",
      "epoch 67: val avg loss is 8512.7392578125\n",
      "epoch 68: total loss is 8512.7392578125, avg loss is 8512.7392578125\n",
      "epoch 68: val avg loss is 8439.48046875\n",
      "epoch 69: total loss is 8439.48046875, avg loss is 8439.48046875\n",
      "epoch 69: val avg loss is 8366.05078125\n",
      "epoch 70: total loss is 8366.05078125, avg loss is 8366.05078125\n",
      "epoch 70: val avg loss is 8301.412109375\n",
      "epoch 71: total loss is 8301.412109375, avg loss is 8301.412109375\n",
      "epoch 71: val avg loss is 8254.544921875\n",
      "epoch 72: total loss is 8254.544921875, avg loss is 8254.544921875\n",
      "epoch 72: val avg loss is 8221.0771484375\n",
      "epoch 73: total loss is 8221.0771484375, avg loss is 8221.0771484375\n",
      "epoch 73: val avg loss is 8215.3828125\n",
      "epoch 74: total loss is 8215.3828125, avg loss is 8215.3828125\n",
      "epoch 74: val avg loss is 8177.1748046875\n",
      "epoch 75: total loss is 8177.1748046875, avg loss is 8177.1748046875\n",
      "epoch 75: val avg loss is 8069.2255859375\n",
      "epoch 76: total loss is 8069.2255859375, avg loss is 8069.2255859375\n",
      "epoch 76: val avg loss is 8025.974609375\n",
      "epoch 77: total loss is 8025.974609375, avg loss is 8025.974609375\n",
      "epoch 77: val avg loss is 7940.9091796875\n",
      "epoch 78: total loss is 7940.9091796875, avg loss is 7940.9091796875\n",
      "epoch 78: val avg loss is 7832.01171875\n",
      "epoch 79: total loss is 7832.01171875, avg loss is 7832.01171875\n",
      "epoch 79: val avg loss is 7836.9150390625\n",
      "epoch 80: total loss is 7836.9150390625, avg loss is 7836.9150390625\n",
      "epoch 80: val avg loss is 7747.25732421875\n",
      "epoch 81: total loss is 7747.25732421875, avg loss is 7747.25732421875\n",
      "epoch 81: val avg loss is 7675.7724609375\n",
      "epoch 82: total loss is 7675.7724609375, avg loss is 7675.7724609375\n",
      "epoch 82: val avg loss is 7605.35888671875\n",
      "epoch 83: total loss is 7605.35888671875, avg loss is 7605.35888671875\n",
      "epoch 83: val avg loss is 7527.318359375\n",
      "epoch 84: total loss is 7527.318359375, avg loss is 7527.318359375\n",
      "epoch 84: val avg loss is 7475.60546875\n",
      "epoch 85: total loss is 7475.60546875, avg loss is 7475.60546875\n",
      "epoch 85: val avg loss is 7416.06640625\n",
      "epoch 86: total loss is 7416.06640625, avg loss is 7416.06640625\n",
      "epoch 86: val avg loss is 7354.85302734375\n",
      "epoch 87: total loss is 7354.85302734375, avg loss is 7354.85302734375\n",
      "epoch 87: val avg loss is 7294.86962890625\n",
      "epoch 88: total loss is 7294.86962890625, avg loss is 7294.86962890625\n",
      "epoch 88: val avg loss is 7237.9990234375\n",
      "epoch 89: total loss is 7237.9990234375, avg loss is 7237.9990234375\n",
      "epoch 89: val avg loss is 7176.28564453125\n",
      "epoch 90: total loss is 7176.28564453125, avg loss is 7176.28564453125\n",
      "epoch 90: val avg loss is 7127.408203125\n",
      "epoch 91: total loss is 7127.408203125, avg loss is 7127.408203125\n",
      "epoch 91: val avg loss is 7065.0234375\n",
      "epoch 92: total loss is 7065.0234375, avg loss is 7065.0234375\n",
      "epoch 92: val avg loss is 7012.533203125\n",
      "epoch 93: total loss is 7012.533203125, avg loss is 7012.533203125\n",
      "epoch 93: val avg loss is 6954.349609375\n",
      "epoch 94: total loss is 6954.349609375, avg loss is 6954.349609375\n",
      "epoch 94: val avg loss is 6902.8828125\n",
      "epoch 95: total loss is 6902.8828125, avg loss is 6902.8828125\n",
      "epoch 95: val avg loss is 6852.79931640625\n",
      "epoch 96: total loss is 6852.79931640625, avg loss is 6852.79931640625\n",
      "epoch 96: val avg loss is 6806.70556640625\n",
      "epoch 97: total loss is 6806.70556640625, avg loss is 6806.70556640625\n",
      "epoch 97: val avg loss is 6776.2431640625\n",
      "epoch 98: total loss is 6776.2431640625, avg loss is 6776.2431640625\n",
      "epoch 98: val avg loss is 6750.794921875\n",
      "epoch 99: total loss is 6750.794921875, avg loss is 6750.794921875\n",
      "epoch 99: val avg loss is 6728.640625\n",
      "epoch 100: total loss is 6728.640625, avg loss is 6728.640625\n",
      "epoch 100: val avg loss is 6667.3740234375\n",
      "epoch 101: total loss is 6667.3740234375, avg loss is 6667.3740234375\n",
      "epoch 101: val avg loss is 6596.5732421875\n",
      "epoch 102: total loss is 6596.5732421875, avg loss is 6596.5732421875\n",
      "epoch 102: val avg loss is 6538.04248046875\n",
      "epoch 103: total loss is 6538.04248046875, avg loss is 6538.04248046875\n",
      "epoch 103: val avg loss is 6448.9462890625\n",
      "epoch 104: total loss is 6448.9462890625, avg loss is 6448.9462890625\n",
      "epoch 104: val avg loss is 6393.68701171875\n",
      "epoch 105: total loss is 6393.68701171875, avg loss is 6393.68701171875\n",
      "epoch 105: val avg loss is 6366.19580078125\n",
      "epoch 106: total loss is 6366.19580078125, avg loss is 6366.19580078125\n",
      "epoch 106: val avg loss is 6284.2119140625\n",
      "epoch 107: total loss is 6284.2119140625, avg loss is 6284.2119140625\n",
      "epoch 107: val avg loss is 6242.21875\n",
      "epoch 108: total loss is 6242.21875, avg loss is 6242.21875\n",
      "epoch 108: val avg loss is 6185.818359375\n",
      "epoch 109: total loss is 6185.818359375, avg loss is 6185.818359375\n",
      "epoch 109: val avg loss is 6137.53857421875\n",
      "epoch 110: total loss is 6137.53857421875, avg loss is 6137.53857421875\n",
      "epoch 110: val avg loss is 6085.8037109375\n",
      "epoch 111: total loss is 6085.8037109375, avg loss is 6085.8037109375\n",
      "epoch 111: val avg loss is 6034.734375\n",
      "epoch 112: total loss is 6034.734375, avg loss is 6034.734375\n",
      "epoch 112: val avg loss is 5981.68212890625\n",
      "epoch 113: total loss is 5981.68212890625, avg loss is 5981.68212890625\n",
      "epoch 113: val avg loss is 5938.33349609375\n",
      "epoch 114: total loss is 5938.33349609375, avg loss is 5938.33349609375\n",
      "epoch 114: val avg loss is 5887.17626953125\n",
      "epoch 115: total loss is 5887.17626953125, avg loss is 5887.17626953125\n",
      "epoch 115: val avg loss is 5841.76025390625\n",
      "epoch 116: total loss is 5841.76025390625, avg loss is 5841.76025390625\n",
      "epoch 116: val avg loss is 5798.75537109375\n",
      "epoch 117: total loss is 5798.75537109375, avg loss is 5798.75537109375\n",
      "epoch 117: val avg loss is 5760.54150390625\n",
      "epoch 118: total loss is 5760.54150390625, avg loss is 5760.54150390625\n",
      "epoch 118: val avg loss is 5723.0849609375\n",
      "epoch 119: total loss is 5723.0849609375, avg loss is 5723.0849609375\n",
      "epoch 119: val avg loss is 5682.869140625\n",
      "epoch 120: total loss is 5682.869140625, avg loss is 5682.869140625\n",
      "epoch 120: val avg loss is 5633.52392578125\n",
      "epoch 121: total loss is 5633.52392578125, avg loss is 5633.52392578125\n",
      "epoch 121: val avg loss is 5575.912109375\n",
      "epoch 122: total loss is 5575.912109375, avg loss is 5575.912109375\n",
      "epoch 122: val avg loss is 5537.072265625\n",
      "epoch 123: total loss is 5537.072265625, avg loss is 5537.072265625\n",
      "epoch 123: val avg loss is 5518.287109375\n",
      "epoch 124: total loss is 5518.287109375, avg loss is 5518.287109375\n",
      "epoch 124: val avg loss is 5485.38134765625\n",
      "epoch 125: total loss is 5485.38134765625, avg loss is 5485.38134765625\n",
      "epoch 125: val avg loss is 5458.9052734375\n",
      "epoch 126: total loss is 5458.9052734375, avg loss is 5458.9052734375\n",
      "epoch 126: val avg loss is 5454.98095703125\n",
      "epoch 127: total loss is 5454.98095703125, avg loss is 5454.98095703125\n",
      "epoch 127: val avg loss is 5444.0087890625\n",
      "epoch 128: total loss is 5444.0087890625, avg loss is 5444.0087890625\n",
      "epoch 128: val avg loss is 5412.5634765625\n",
      "epoch 129: total loss is 5412.5634765625, avg loss is 5412.5634765625\n",
      "epoch 129: val avg loss is 5317.2890625\n",
      "epoch 130: total loss is 5317.2890625, avg loss is 5317.2890625\n",
      "epoch 130: val avg loss is 5249.2373046875\n",
      "epoch 131: total loss is 5249.2373046875, avg loss is 5249.2373046875\n",
      "epoch 131: val avg loss is 5224.76220703125\n",
      "epoch 132: total loss is 5224.76220703125, avg loss is 5224.76220703125\n",
      "epoch 132: val avg loss is 5185.22998046875\n",
      "epoch 133: total loss is 5185.22998046875, avg loss is 5185.22998046875\n",
      "epoch 133: val avg loss is 5189.5732421875\n",
      "epoch 134: total loss is 5189.5732421875, avg loss is 5189.5732421875\n",
      "epoch 134: val avg loss is 5201.14404296875\n",
      "epoch 135: total loss is 5201.14404296875, avg loss is 5201.14404296875\n",
      "epoch 135: val avg loss is 5083.912109375\n",
      "epoch 136: total loss is 5083.912109375, avg loss is 5083.912109375\n",
      "epoch 136: val avg loss is 4981.021484375\n",
      "epoch 137: total loss is 4981.021484375, avg loss is 4981.021484375\n",
      "epoch 137: val avg loss is 4971.53515625\n",
      "epoch 138: total loss is 4971.53515625, avg loss is 4971.53515625\n",
      "epoch 138: val avg loss is 4891.76025390625\n",
      "epoch 139: total loss is 4891.76025390625, avg loss is 4891.76025390625\n",
      "epoch 139: val avg loss is 4862.5341796875\n",
      "epoch 140: total loss is 4862.5341796875, avg loss is 4862.5341796875\n",
      "epoch 140: val avg loss is 4795.3544921875\n",
      "epoch 141: total loss is 4795.3544921875, avg loss is 4795.3544921875\n",
      "epoch 141: val avg loss is 4769.3017578125\n",
      "epoch 142: total loss is 4769.3017578125, avg loss is 4769.3017578125\n",
      "epoch 142: val avg loss is 4714.541015625\n",
      "epoch 143: total loss is 4714.541015625, avg loss is 4714.541015625\n",
      "epoch 143: val avg loss is 4678.59912109375\n",
      "epoch 144: total loss is 4678.59912109375, avg loss is 4678.59912109375\n",
      "epoch 144: val avg loss is 4637.3095703125\n",
      "epoch 145: total loss is 4637.3095703125, avg loss is 4637.3095703125\n",
      "epoch 145: val avg loss is 4589.66259765625\n",
      "epoch 146: total loss is 4589.66259765625, avg loss is 4589.66259765625\n",
      "epoch 146: val avg loss is 4553.474609375\n",
      "epoch 147: total loss is 4553.474609375, avg loss is 4553.474609375\n",
      "epoch 147: val avg loss is 4514.7177734375\n",
      "epoch 148: total loss is 4514.7177734375, avg loss is 4514.7177734375\n",
      "epoch 148: val avg loss is 4471.857421875\n",
      "epoch 149: total loss is 4471.857421875, avg loss is 4471.857421875\n",
      "epoch 149: val avg loss is 4434.3837890625\n",
      "epoch 150: total loss is 4434.3837890625, avg loss is 4434.3837890625\n",
      "epoch 150: val avg loss is 4396.76513671875\n",
      "epoch 151: total loss is 4396.76513671875, avg loss is 4396.76513671875\n",
      "epoch 151: val avg loss is 4356.53759765625\n",
      "epoch 152: total loss is 4356.53759765625, avg loss is 4356.53759765625\n",
      "epoch 152: val avg loss is 4320.66552734375\n",
      "epoch 153: total loss is 4320.66552734375, avg loss is 4320.66552734375\n",
      "epoch 153: val avg loss is 4284.19140625\n",
      "epoch 154: total loss is 4284.19140625, avg loss is 4284.19140625\n",
      "epoch 154: val avg loss is 4246.87744140625\n",
      "epoch 155: total loss is 4246.87744140625, avg loss is 4246.87744140625\n",
      "epoch 155: val avg loss is 4209.71484375\n",
      "epoch 156: total loss is 4209.71484375, avg loss is 4209.71484375\n",
      "epoch 156: val avg loss is 4174.85888671875\n",
      "epoch 157: total loss is 4174.85888671875, avg loss is 4174.85888671875\n",
      "epoch 157: val avg loss is 4138.89599609375\n",
      "epoch 158: total loss is 4138.89599609375, avg loss is 4138.89599609375\n",
      "epoch 158: val avg loss is 4104.58203125\n",
      "epoch 159: total loss is 4104.58203125, avg loss is 4104.58203125\n",
      "epoch 159: val avg loss is 4069.419677734375\n",
      "epoch 160: total loss is 4069.419677734375, avg loss is 4069.419677734375\n",
      "epoch 160: val avg loss is 4038.35009765625\n",
      "epoch 161: total loss is 4038.35009765625, avg loss is 4038.35009765625\n",
      "epoch 161: val avg loss is 4009.36865234375\n",
      "epoch 162: total loss is 4009.36865234375, avg loss is 4009.36865234375\n",
      "epoch 162: val avg loss is 3988.923583984375\n",
      "epoch 163: total loss is 3988.923583984375, avg loss is 3988.923583984375\n",
      "epoch 163: val avg loss is 3977.74658203125\n",
      "epoch 164: total loss is 3977.74658203125, avg loss is 3977.74658203125\n",
      "epoch 164: val avg loss is 3960.623291015625\n",
      "epoch 165: total loss is 3960.623291015625, avg loss is 3960.623291015625\n",
      "epoch 165: val avg loss is 3910.05224609375\n",
      "epoch 166: total loss is 3910.05224609375, avg loss is 3910.05224609375\n",
      "epoch 166: val avg loss is 3856.50537109375\n",
      "epoch 167: total loss is 3856.50537109375, avg loss is 3856.50537109375\n",
      "epoch 167: val avg loss is 3857.7216796875\n",
      "epoch 168: total loss is 3857.7216796875, avg loss is 3857.7216796875\n",
      "epoch 168: val avg loss is 3843.41357421875\n",
      "epoch 169: total loss is 3843.41357421875, avg loss is 3843.41357421875\n",
      "epoch 169: val avg loss is 3812.231689453125\n",
      "epoch 170: total loss is 3812.231689453125, avg loss is 3812.231689453125\n",
      "epoch 170: val avg loss is 3851.91064453125\n",
      "epoch 171: total loss is 3851.91064453125, avg loss is 3851.91064453125\n",
      "epoch 171: val avg loss is 3921.236083984375\n",
      "epoch 172: total loss is 3921.236083984375, avg loss is 3921.236083984375\n",
      "epoch 172: val avg loss is 3847.171875\n",
      "epoch 173: total loss is 3847.171875, avg loss is 3847.171875\n",
      "epoch 173: val avg loss is 3663.831298828125\n",
      "epoch 174: total loss is 3663.831298828125, avg loss is 3663.831298828125\n",
      "epoch 174: val avg loss is 3686.99609375\n",
      "epoch 175: total loss is 3686.99609375, avg loss is 3686.99609375\n",
      "epoch 175: val avg loss is 3625.37841796875\n",
      "epoch 176: total loss is 3625.37841796875, avg loss is 3625.37841796875\n",
      "epoch 176: val avg loss is 3597.5\n",
      "epoch 177: total loss is 3597.5, avg loss is 3597.5\n",
      "epoch 177: val avg loss is 3550.04296875\n",
      "epoch 178: total loss is 3550.04296875, avg loss is 3550.04296875\n",
      "epoch 178: val avg loss is 3498.92431640625\n",
      "epoch 179: total loss is 3498.92431640625, avg loss is 3498.92431640625\n",
      "epoch 179: val avg loss is 3471.07861328125\n",
      "epoch 180: total loss is 3471.07861328125, avg loss is 3471.07861328125\n",
      "epoch 180: val avg loss is 3437.779052734375\n",
      "epoch 181: total loss is 3437.779052734375, avg loss is 3437.779052734375\n",
      "epoch 181: val avg loss is 3402.3623046875\n",
      "epoch 182: total loss is 3402.3623046875, avg loss is 3402.3623046875\n",
      "epoch 182: val avg loss is 3369.7529296875\n",
      "epoch 183: total loss is 3369.7529296875, avg loss is 3369.7529296875\n",
      "epoch 183: val avg loss is 3336.712158203125\n",
      "epoch 184: total loss is 3336.712158203125, avg loss is 3336.712158203125\n",
      "epoch 184: val avg loss is 3310.052734375\n",
      "epoch 185: total loss is 3310.052734375, avg loss is 3310.052734375\n",
      "epoch 185: val avg loss is 3279.0810546875\n",
      "epoch 186: total loss is 3279.0810546875, avg loss is 3279.0810546875\n",
      "epoch 186: val avg loss is 3245.376953125\n",
      "epoch 187: total loss is 3245.376953125, avg loss is 3245.376953125\n",
      "epoch 187: val avg loss is 3218.546875\n",
      "epoch 188: total loss is 3218.546875, avg loss is 3218.546875\n",
      "epoch 188: val avg loss is 3190.1640625\n",
      "epoch 189: total loss is 3190.1640625, avg loss is 3190.1640625\n",
      "epoch 189: val avg loss is 3157.8984375\n",
      "epoch 190: total loss is 3157.8984375, avg loss is 3157.8984375\n",
      "epoch 190: val avg loss is 3128.98828125\n",
      "epoch 191: total loss is 3128.98828125, avg loss is 3128.98828125\n",
      "epoch 191: val avg loss is 3098.034912109375\n",
      "epoch 192: total loss is 3098.034912109375, avg loss is 3098.034912109375\n",
      "epoch 192: val avg loss is 3065.997314453125\n",
      "epoch 193: total loss is 3065.997314453125, avg loss is 3065.997314453125\n",
      "epoch 193: val avg loss is 3040.855712890625\n",
      "epoch 194: total loss is 3040.855712890625, avg loss is 3040.855712890625\n",
      "epoch 194: val avg loss is 3013.05078125\n",
      "epoch 195: total loss is 3013.05078125, avg loss is 3013.05078125\n",
      "epoch 195: val avg loss is 2987.26025390625\n",
      "epoch 196: total loss is 2987.26025390625, avg loss is 2987.26025390625\n",
      "epoch 196: val avg loss is 2959.04541015625\n",
      "epoch 197: total loss is 2959.04541015625, avg loss is 2959.04541015625\n",
      "epoch 197: val avg loss is 2931.42431640625\n",
      "epoch 198: total loss is 2931.42431640625, avg loss is 2931.42431640625\n",
      "epoch 198: val avg loss is 2904.054931640625\n",
      "epoch 199: total loss is 2904.054931640625, avg loss is 2904.054931640625\n",
      "epoch 199: val avg loss is 2879.023193359375\n",
      "epoch 200: total loss is 2879.023193359375, avg loss is 2879.023193359375\n",
      "epoch 200: val avg loss is 2854.534912109375\n",
      "epoch 201: total loss is 2854.534912109375, avg loss is 2854.534912109375\n",
      "epoch 201: val avg loss is 2830.693359375\n",
      "epoch 202: total loss is 2830.693359375, avg loss is 2830.693359375\n",
      "epoch 202: val avg loss is 2810.199951171875\n",
      "epoch 203: total loss is 2810.199951171875, avg loss is 2810.199951171875\n",
      "epoch 203: val avg loss is 2792.546630859375\n",
      "epoch 204: total loss is 2792.546630859375, avg loss is 2792.546630859375\n",
      "epoch 204: val avg loss is 2782.866943359375\n",
      "epoch 205: total loss is 2782.866943359375, avg loss is 2782.866943359375\n",
      "epoch 205: val avg loss is 2783.110107421875\n",
      "epoch 206: total loss is 2783.110107421875, avg loss is 2783.110107421875\n",
      "epoch 206: val avg loss is 2780.02001953125\n",
      "epoch 207: total loss is 2780.02001953125, avg loss is 2780.02001953125\n",
      "epoch 207: val avg loss is 2747.737548828125\n",
      "epoch 208: total loss is 2747.737548828125, avg loss is 2747.737548828125\n",
      "epoch 208: val avg loss is 2680.891845703125\n",
      "epoch 209: total loss is 2680.891845703125, avg loss is 2680.891845703125\n",
      "epoch 209: val avg loss is 2638.4228515625\n",
      "epoch 210: total loss is 2638.4228515625, avg loss is 2638.4228515625\n",
      "epoch 210: val avg loss is 2632.882080078125\n",
      "epoch 211: total loss is 2632.882080078125, avg loss is 2632.882080078125\n",
      "epoch 211: val avg loss is 2611.7685546875\n",
      "epoch 212: total loss is 2611.7685546875, avg loss is 2611.7685546875\n",
      "epoch 212: val avg loss is 2584.603271484375\n",
      "epoch 213: total loss is 2584.603271484375, avg loss is 2584.603271484375\n",
      "epoch 213: val avg loss is 2573.5126953125\n",
      "epoch 214: total loss is 2573.5126953125, avg loss is 2573.5126953125\n",
      "epoch 214: val avg loss is 2570.999267578125\n",
      "epoch 215: total loss is 2570.999267578125, avg loss is 2570.999267578125\n",
      "epoch 215: val avg loss is 2568.677734375\n",
      "epoch 216: total loss is 2568.677734375, avg loss is 2568.677734375\n",
      "epoch 216: val avg loss is 2536.090087890625\n",
      "epoch 217: total loss is 2536.090087890625, avg loss is 2536.090087890625\n",
      "epoch 217: val avg loss is 2476.4130859375\n",
      "epoch 218: total loss is 2476.4130859375, avg loss is 2476.4130859375\n",
      "epoch 218: val avg loss is 2449.782958984375\n",
      "epoch 219: total loss is 2449.782958984375, avg loss is 2449.782958984375\n",
      "epoch 219: val avg loss is 2444.6064453125\n",
      "epoch 220: total loss is 2444.6064453125, avg loss is 2444.6064453125\n",
      "epoch 220: val avg loss is 2414.17041015625\n",
      "epoch 221: total loss is 2414.17041015625, avg loss is 2414.17041015625\n",
      "epoch 221: val avg loss is 2397.708740234375\n",
      "epoch 222: total loss is 2397.708740234375, avg loss is 2397.708740234375\n",
      "epoch 222: val avg loss is 2385.097412109375\n",
      "epoch 223: total loss is 2385.097412109375, avg loss is 2385.097412109375\n",
      "epoch 223: val avg loss is 2387.6552734375\n",
      "epoch 224: total loss is 2387.6552734375, avg loss is 2387.6552734375\n",
      "epoch 224: val avg loss is 2418.24755859375\n",
      "epoch 225: total loss is 2418.24755859375, avg loss is 2418.24755859375\n",
      "epoch 225: val avg loss is 2425.771240234375\n",
      "epoch 226: total loss is 2425.771240234375, avg loss is 2425.771240234375\n",
      "epoch 226: val avg loss is 2361.968017578125\n",
      "epoch 227: total loss is 2361.968017578125, avg loss is 2361.968017578125\n",
      "epoch 227: val avg loss is 2291.60595703125\n",
      "epoch 228: total loss is 2291.60595703125, avg loss is 2291.60595703125\n",
      "epoch 228: val avg loss is 2277.802001953125\n",
      "epoch 229: total loss is 2277.802001953125, avg loss is 2277.802001953125\n",
      "epoch 229: val avg loss is 2273.94091796875\n",
      "epoch 230: total loss is 2273.94091796875, avg loss is 2273.94091796875\n",
      "epoch 230: val avg loss is 2249.992431640625\n",
      "epoch 231: total loss is 2249.992431640625, avg loss is 2249.992431640625\n",
      "epoch 231: val avg loss is 2192.52734375\n",
      "epoch 232: total loss is 2192.52734375, avg loss is 2192.52734375\n",
      "epoch 232: val avg loss is 2172.65283203125\n",
      "epoch 233: total loss is 2172.65283203125, avg loss is 2172.65283203125\n",
      "epoch 233: val avg loss is 2157.90087890625\n",
      "epoch 234: total loss is 2157.90087890625, avg loss is 2157.90087890625\n",
      "epoch 234: val avg loss is 2116.055908203125\n",
      "epoch 235: total loss is 2116.055908203125, avg loss is 2116.055908203125\n",
      "epoch 235: val avg loss is 2107.64892578125\n",
      "epoch 236: total loss is 2107.64892578125, avg loss is 2107.64892578125\n",
      "epoch 236: val avg loss is 2081.7509765625\n",
      "epoch 237: total loss is 2081.7509765625, avg loss is 2081.7509765625\n",
      "epoch 237: val avg loss is 2049.502685546875\n",
      "epoch 238: total loss is 2049.502685546875, avg loss is 2049.502685546875\n",
      "epoch 238: val avg loss is 2037.165771484375\n",
      "epoch 239: total loss is 2037.165771484375, avg loss is 2037.165771484375\n",
      "epoch 239: val avg loss is 2011.2965087890625\n",
      "epoch 240: total loss is 2011.2965087890625, avg loss is 2011.2965087890625\n",
      "epoch 240: val avg loss is 1996.2222900390625\n",
      "epoch 241: total loss is 1996.2222900390625, avg loss is 1996.2222900390625\n",
      "epoch 241: val avg loss is 1972.004638671875\n",
      "epoch 242: total loss is 1972.004638671875, avg loss is 1972.004638671875\n",
      "epoch 242: val avg loss is 1956.0267333984375\n",
      "epoch 243: total loss is 1956.0267333984375, avg loss is 1956.0267333984375\n",
      "epoch 243: val avg loss is 1932.455322265625\n",
      "epoch 244: total loss is 1932.455322265625, avg loss is 1932.455322265625\n",
      "epoch 244: val avg loss is 1916.6409912109375\n",
      "epoch 245: total loss is 1916.6409912109375, avg loss is 1916.6409912109375\n",
      "epoch 245: val avg loss is 1894.7432861328125\n",
      "epoch 246: total loss is 1894.7432861328125, avg loss is 1894.7432861328125\n",
      "epoch 246: val avg loss is 1878.6566162109375\n",
      "epoch 247: total loss is 1878.6566162109375, avg loss is 1878.6566162109375\n",
      "epoch 247: val avg loss is 1859.5843505859375\n",
      "epoch 248: total loss is 1859.5843505859375, avg loss is 1859.5843505859375\n",
      "epoch 248: val avg loss is 1840.781982421875\n",
      "epoch 249: total loss is 1840.781982421875, avg loss is 1840.781982421875\n",
      "epoch 249: val avg loss is 1824.86572265625\n",
      "epoch 250: total loss is 1824.86572265625, avg loss is 1824.86572265625\n",
      "epoch 250: val avg loss is 1807.213623046875\n",
      "epoch 251: total loss is 1807.213623046875, avg loss is 1807.213623046875\n",
      "epoch 251: val avg loss is 1790.8016357421875\n",
      "epoch 252: total loss is 1790.8016357421875, avg loss is 1790.8016357421875\n",
      "epoch 252: val avg loss is 1772.8441162109375\n",
      "epoch 253: total loss is 1772.8441162109375, avg loss is 1772.8441162109375\n",
      "epoch 253: val avg loss is 1758.3294677734375\n",
      "epoch 254: total loss is 1758.3294677734375, avg loss is 1758.3294677734375\n",
      "epoch 254: val avg loss is 1742.0767822265625\n",
      "epoch 255: total loss is 1742.0767822265625, avg loss is 1742.0767822265625\n",
      "epoch 255: val avg loss is 1726.785888671875\n",
      "epoch 256: total loss is 1726.785888671875, avg loss is 1726.785888671875\n",
      "epoch 256: val avg loss is 1711.4896240234375\n",
      "epoch 257: total loss is 1711.4896240234375, avg loss is 1711.4896240234375\n",
      "epoch 257: val avg loss is 1696.841064453125\n",
      "epoch 258: total loss is 1696.841064453125, avg loss is 1696.841064453125\n",
      "epoch 258: val avg loss is 1680.5982666015625\n",
      "epoch 259: total loss is 1680.5982666015625, avg loss is 1680.5982666015625\n",
      "epoch 259: val avg loss is 1662.9281005859375\n",
      "epoch 260: total loss is 1662.9281005859375, avg loss is 1662.9281005859375\n",
      "epoch 260: val avg loss is 1648.03564453125\n",
      "epoch 261: total loss is 1648.03564453125, avg loss is 1648.03564453125\n",
      "epoch 261: val avg loss is 1638.5738525390625\n",
      "epoch 262: total loss is 1638.5738525390625, avg loss is 1638.5738525390625\n",
      "epoch 262: val avg loss is 1638.85595703125\n",
      "epoch 263: total loss is 1638.85595703125, avg loss is 1638.85595703125\n",
      "epoch 263: val avg loss is 1648.9664306640625\n",
      "epoch 264: total loss is 1648.9664306640625, avg loss is 1648.9664306640625\n",
      "epoch 264: val avg loss is 1665.119384765625\n",
      "epoch 265: total loss is 1665.119384765625, avg loss is 1665.119384765625\n",
      "epoch 265: val avg loss is 1679.038818359375\n",
      "epoch 266: total loss is 1679.038818359375, avg loss is 1679.038818359375\n",
      "epoch 266: val avg loss is 1687.618408203125\n",
      "epoch 267: total loss is 1687.618408203125, avg loss is 1687.618408203125\n",
      "epoch 267: val avg loss is 1672.819091796875\n",
      "epoch 268: total loss is 1672.819091796875, avg loss is 1672.819091796875\n",
      "epoch 268: val avg loss is 1624.1064453125\n",
      "epoch 269: total loss is 1624.1064453125, avg loss is 1624.1064453125\n",
      "epoch 269: val avg loss is 1612.3099365234375\n",
      "epoch 270: total loss is 1612.3099365234375, avg loss is 1612.3099365234375\n",
      "epoch 270: val avg loss is 1649.8841552734375\n",
      "epoch 271: total loss is 1649.8841552734375, avg loss is 1649.8841552734375\n",
      "epoch 271: val avg loss is 1627.4200439453125\n",
      "epoch 272: total loss is 1627.4200439453125, avg loss is 1627.4200439453125\n",
      "epoch 272: val avg loss is 1582.3870849609375\n",
      "epoch 273: total loss is 1582.3870849609375, avg loss is 1582.3870849609375\n",
      "epoch 273: val avg loss is 1552.9063720703125\n",
      "epoch 274: total loss is 1552.9063720703125, avg loss is 1552.9063720703125\n",
      "epoch 274: val avg loss is 1511.334228515625\n",
      "epoch 275: total loss is 1511.334228515625, avg loss is 1511.334228515625\n",
      "epoch 275: val avg loss is 1477.9217529296875\n",
      "epoch 276: total loss is 1477.9217529296875, avg loss is 1477.9217529296875\n",
      "epoch 276: val avg loss is 1458.1270751953125\n",
      "epoch 277: total loss is 1458.1270751953125, avg loss is 1458.1270751953125\n",
      "epoch 277: val avg loss is 1434.781494140625\n",
      "epoch 278: total loss is 1434.781494140625, avg loss is 1434.781494140625\n",
      "epoch 278: val avg loss is 1425.3375244140625\n",
      "epoch 279: total loss is 1425.3375244140625, avg loss is 1425.3375244140625\n",
      "epoch 279: val avg loss is 1396.34375\n",
      "epoch 280: total loss is 1396.34375, avg loss is 1396.34375\n",
      "epoch 280: val avg loss is 1384.465087890625\n",
      "epoch 281: total loss is 1384.465087890625, avg loss is 1384.465087890625\n",
      "epoch 281: val avg loss is 1366.895263671875\n",
      "epoch 282: total loss is 1366.895263671875, avg loss is 1366.895263671875\n",
      "epoch 282: val avg loss is 1347.99658203125\n",
      "epoch 283: total loss is 1347.99658203125, avg loss is 1347.99658203125\n",
      "epoch 283: val avg loss is 1339.0244140625\n",
      "epoch 284: total loss is 1339.0244140625, avg loss is 1339.0244140625\n",
      "epoch 284: val avg loss is 1316.5894775390625\n",
      "epoch 285: total loss is 1316.5894775390625, avg loss is 1316.5894775390625\n",
      "epoch 285: val avg loss is 1306.105224609375\n",
      "epoch 286: total loss is 1306.105224609375, avg loss is 1306.105224609375\n",
      "epoch 286: val avg loss is 1290.1715087890625\n",
      "epoch 287: total loss is 1290.1715087890625, avg loss is 1290.1715087890625\n",
      "epoch 287: val avg loss is 1277.1297607421875\n",
      "epoch 288: total loss is 1277.1297607421875, avg loss is 1277.1297607421875\n",
      "epoch 288: val avg loss is 1264.9715576171875\n",
      "epoch 289: total loss is 1264.9715576171875, avg loss is 1264.9715576171875\n",
      "epoch 289: val avg loss is 1246.94482421875\n",
      "epoch 290: total loss is 1246.94482421875, avg loss is 1246.94482421875\n",
      "epoch 290: val avg loss is 1237.322021484375\n",
      "epoch 291: total loss is 1237.322021484375, avg loss is 1237.322021484375\n",
      "epoch 291: val avg loss is 1224.0792236328125\n",
      "epoch 292: total loss is 1224.0792236328125, avg loss is 1224.0792236328125\n",
      "epoch 292: val avg loss is 1210.3182373046875\n",
      "epoch 293: total loss is 1210.3182373046875, avg loss is 1210.3182373046875\n",
      "epoch 293: val avg loss is 1198.628173828125\n",
      "epoch 294: total loss is 1198.628173828125, avg loss is 1198.628173828125\n",
      "epoch 294: val avg loss is 1185.3104248046875\n",
      "epoch 295: total loss is 1185.3104248046875, avg loss is 1185.3104248046875\n",
      "epoch 295: val avg loss is 1174.493896484375\n",
      "epoch 296: total loss is 1174.493896484375, avg loss is 1174.493896484375\n",
      "epoch 296: val avg loss is 1161.740234375\n",
      "epoch 297: total loss is 1161.740234375, avg loss is 1161.740234375\n",
      "epoch 297: val avg loss is 1150.6766357421875\n",
      "epoch 298: total loss is 1150.6766357421875, avg loss is 1150.6766357421875\n",
      "epoch 298: val avg loss is 1140.06982421875\n",
      "epoch 299: total loss is 1140.06982421875, avg loss is 1140.06982421875\n",
      "epoch 299: val avg loss is 1131.285400390625\n",
      "epoch 300: total loss is 1131.285400390625, avg loss is 1131.285400390625\n",
      "epoch 300: val avg loss is 1124.375\n",
      "epoch 301: total loss is 1124.375, avg loss is 1124.375\n",
      "epoch 301: val avg loss is 1123.9049072265625\n",
      "epoch 302: total loss is 1123.9049072265625, avg loss is 1123.9049072265625\n",
      "epoch 302: val avg loss is 1134.697998046875\n",
      "epoch 303: total loss is 1134.697998046875, avg loss is 1134.697998046875\n",
      "epoch 303: val avg loss is 1160.958984375\n",
      "epoch 304: total loss is 1160.958984375, avg loss is 1160.958984375\n",
      "epoch 304: val avg loss is 1186.9075927734375\n",
      "epoch 305: total loss is 1186.9075927734375, avg loss is 1186.9075927734375\n",
      "epoch 305: val avg loss is 1162.0889892578125\n",
      "epoch 306: total loss is 1162.0889892578125, avg loss is 1162.0889892578125\n",
      "epoch 306: val avg loss is 1084.2537841796875\n",
      "epoch 307: total loss is 1084.2537841796875, avg loss is 1084.2537841796875\n",
      "epoch 307: val avg loss is 1056.3062744140625\n",
      "epoch 308: total loss is 1056.3062744140625, avg loss is 1056.3062744140625\n",
      "epoch 308: val avg loss is 1068.54052734375\n",
      "epoch 309: total loss is 1068.54052734375, avg loss is 1068.54052734375\n",
      "epoch 309: val avg loss is 1045.5882568359375\n",
      "epoch 310: total loss is 1045.5882568359375, avg loss is 1045.5882568359375\n",
      "epoch 310: val avg loss is 1029.7576904296875\n",
      "epoch 311: total loss is 1029.7576904296875, avg loss is 1029.7576904296875\n",
      "epoch 311: val avg loss is 1017.22998046875\n",
      "epoch 312: total loss is 1017.22998046875, avg loss is 1017.22998046875\n",
      "epoch 312: val avg loss is 1004.3388671875\n",
      "epoch 313: total loss is 1004.3388671875, avg loss is 1004.3388671875\n",
      "epoch 313: val avg loss is 995.7150268554688\n",
      "epoch 314: total loss is 995.7150268554688, avg loss is 995.7150268554688\n",
      "epoch 314: val avg loss is 986.1122436523438\n",
      "epoch 315: total loss is 986.1122436523438, avg loss is 986.1122436523438\n",
      "epoch 315: val avg loss is 968.3827514648438\n",
      "epoch 316: total loss is 968.3827514648438, avg loss is 968.3827514648438\n",
      "epoch 316: val avg loss is 960.695068359375\n",
      "epoch 317: total loss is 960.695068359375, avg loss is 960.695068359375\n",
      "epoch 317: val avg loss is 951.5878295898438\n",
      "epoch 318: total loss is 951.5878295898438, avg loss is 951.5878295898438\n",
      "epoch 318: val avg loss is 938.0834350585938\n",
      "epoch 319: total loss is 938.0834350585938, avg loss is 938.0834350585938\n",
      "epoch 319: val avg loss is 929.9832153320312\n",
      "epoch 320: total loss is 929.9832153320312, avg loss is 929.9832153320312\n",
      "epoch 320: val avg loss is 918.4680786132812\n",
      "epoch 321: total loss is 918.4680786132812, avg loss is 918.4680786132812\n",
      "epoch 321: val avg loss is 911.510498046875\n",
      "epoch 322: total loss is 911.510498046875, avg loss is 911.510498046875\n",
      "epoch 322: val avg loss is 899.8463134765625\n",
      "epoch 323: total loss is 899.8463134765625, avg loss is 899.8463134765625\n",
      "epoch 323: val avg loss is 889.5936889648438\n",
      "epoch 324: total loss is 889.5936889648438, avg loss is 889.5936889648438\n",
      "epoch 324: val avg loss is 882.0819702148438\n",
      "epoch 325: total loss is 882.0819702148438, avg loss is 882.0819702148438\n",
      "epoch 325: val avg loss is 871.8624267578125\n",
      "epoch 326: total loss is 871.8624267578125, avg loss is 871.8624267578125\n",
      "epoch 326: val avg loss is 863.1610717773438\n",
      "epoch 327: total loss is 863.1610717773438, avg loss is 863.1610717773438\n",
      "epoch 327: val avg loss is 854.0006713867188\n",
      "epoch 328: total loss is 854.0006713867188, avg loss is 854.0006713867188\n",
      "epoch 328: val avg loss is 846.3924560546875\n",
      "epoch 329: total loss is 846.3924560546875, avg loss is 846.3924560546875\n",
      "epoch 329: val avg loss is 836.30419921875\n",
      "epoch 330: total loss is 836.30419921875, avg loss is 836.30419921875\n",
      "epoch 330: val avg loss is 827.8927612304688\n",
      "epoch 331: total loss is 827.8927612304688, avg loss is 827.8927612304688\n",
      "epoch 331: val avg loss is 820.0770874023438\n",
      "epoch 332: total loss is 820.0770874023438, avg loss is 820.0770874023438\n",
      "epoch 332: val avg loss is 811.2142333984375\n",
      "epoch 333: total loss is 811.2142333984375, avg loss is 811.2142333984375\n",
      "epoch 333: val avg loss is 802.9965209960938\n",
      "epoch 334: total loss is 802.9965209960938, avg loss is 802.9965209960938\n",
      "epoch 334: val avg loss is 795.23193359375\n",
      "epoch 335: total loss is 795.23193359375, avg loss is 795.23193359375\n",
      "epoch 335: val avg loss is 787.0164794921875\n",
      "epoch 336: total loss is 787.0164794921875, avg loss is 787.0164794921875\n",
      "epoch 336: val avg loss is 779.058837890625\n",
      "epoch 337: total loss is 779.058837890625, avg loss is 779.058837890625\n",
      "epoch 337: val avg loss is 771.3414916992188\n",
      "epoch 338: total loss is 771.3414916992188, avg loss is 771.3414916992188\n",
      "epoch 338: val avg loss is 764.0221557617188\n",
      "epoch 339: total loss is 764.0221557617188, avg loss is 764.0221557617188\n",
      "epoch 339: val avg loss is 756.501953125\n",
      "epoch 340: total loss is 756.501953125, avg loss is 756.501953125\n",
      "epoch 340: val avg loss is 749.876220703125\n",
      "epoch 341: total loss is 749.876220703125, avg loss is 749.876220703125\n",
      "epoch 341: val avg loss is 744.5530395507812\n",
      "epoch 342: total loss is 744.5530395507812, avg loss is 744.5530395507812\n",
      "epoch 342: val avg loss is 741.2354125976562\n",
      "epoch 343: total loss is 741.2354125976562, avg loss is 741.2354125976562\n",
      "epoch 343: val avg loss is 742.333740234375\n",
      "epoch 344: total loss is 742.333740234375, avg loss is 742.333740234375\n",
      "epoch 344: val avg loss is 748.832275390625\n",
      "epoch 345: total loss is 748.832275390625, avg loss is 748.832275390625\n",
      "epoch 345: val avg loss is 757.1585693359375\n",
      "epoch 346: total loss is 757.1585693359375, avg loss is 757.1585693359375\n",
      "epoch 346: val avg loss is 749.5521240234375\n",
      "epoch 347: total loss is 749.5521240234375, avg loss is 749.5521240234375\n",
      "epoch 347: val avg loss is 721.3392944335938\n",
      "epoch 348: total loss is 721.3392944335938, avg loss is 721.3392944335938\n",
      "epoch 348: val avg loss is 713.0347900390625\n",
      "epoch 349: total loss is 713.0347900390625, avg loss is 713.0347900390625\n",
      "epoch 349: val avg loss is 732.28271484375\n",
      "epoch 350: total loss is 732.28271484375, avg loss is 732.28271484375\n",
      "epoch 350: val avg loss is 729.6954956054688\n",
      "epoch 351: total loss is 729.6954956054688, avg loss is 729.6954956054688\n",
      "epoch 351: val avg loss is 717.1173706054688\n",
      "epoch 352: total loss is 717.1173706054688, avg loss is 717.1173706054688\n",
      "epoch 352: val avg loss is 692.3123779296875\n",
      "epoch 353: total loss is 692.3123779296875, avg loss is 692.3123779296875\n",
      "epoch 353: val avg loss is 667.3677368164062\n",
      "epoch 354: total loss is 667.3677368164062, avg loss is 667.3677368164062\n",
      "epoch 354: val avg loss is 672.6405029296875\n",
      "epoch 355: total loss is 672.6405029296875, avg loss is 672.6405029296875\n",
      "epoch 355: val avg loss is 658.8734741210938\n",
      "epoch 356: total loss is 658.8734741210938, avg loss is 658.8734741210938\n",
      "epoch 356: val avg loss is 650.7106323242188\n",
      "epoch 357: total loss is 650.7106323242188, avg loss is 650.7106323242188\n",
      "epoch 357: val avg loss is 641.973388671875\n",
      "epoch 358: total loss is 641.973388671875, avg loss is 641.973388671875\n",
      "epoch 358: val avg loss is 634.006591796875\n",
      "epoch 359: total loss is 634.006591796875, avg loss is 634.006591796875\n",
      "epoch 359: val avg loss is 628.9782104492188\n",
      "epoch 360: total loss is 628.9782104492188, avg loss is 628.9782104492188\n",
      "epoch 360: val avg loss is 620.5965576171875\n",
      "epoch 361: total loss is 620.5965576171875, avg loss is 620.5965576171875\n",
      "epoch 361: val avg loss is 616.2100219726562\n",
      "epoch 362: total loss is 616.2100219726562, avg loss is 616.2100219726562\n",
      "epoch 362: val avg loss is 607.2001342773438\n",
      "epoch 363: total loss is 607.2001342773438, avg loss is 607.2001342773438\n",
      "epoch 363: val avg loss is 605.2723999023438\n",
      "epoch 364: total loss is 605.2723999023438, avg loss is 605.2723999023438\n",
      "epoch 364: val avg loss is 599.3519287109375\n",
      "epoch 365: total loss is 599.3519287109375, avg loss is 599.3519287109375\n",
      "epoch 365: val avg loss is 600.2591552734375\n",
      "epoch 366: total loss is 600.2591552734375, avg loss is 600.2591552734375\n",
      "epoch 366: val avg loss is 602.0992431640625\n",
      "epoch 367: total loss is 602.0992431640625, avg loss is 602.0992431640625\n",
      "epoch 367: val avg loss is 615.5095825195312\n",
      "epoch 368: total loss is 615.5095825195312, avg loss is 615.5095825195312\n",
      "epoch 368: val avg loss is 631.7899169921875\n",
      "epoch 369: total loss is 631.7899169921875, avg loss is 631.7899169921875\n",
      "epoch 369: val avg loss is 645.06787109375\n",
      "epoch 370: total loss is 645.06787109375, avg loss is 645.06787109375\n",
      "epoch 370: val avg loss is 630.6419677734375\n",
      "epoch 371: total loss is 630.6419677734375, avg loss is 630.6419677734375\n",
      "epoch 371: val avg loss is 593.639404296875\n",
      "epoch 372: total loss is 593.639404296875, avg loss is 593.639404296875\n",
      "epoch 372: val avg loss is 579.8595581054688\n",
      "epoch 373: total loss is 579.8595581054688, avg loss is 579.8595581054688\n",
      "epoch 373: val avg loss is 602.7868041992188\n",
      "epoch 374: total loss is 602.7868041992188, avg loss is 602.7868041992188\n",
      "epoch 374: val avg loss is 608.8834228515625\n",
      "epoch 375: total loss is 608.8834228515625, avg loss is 608.8834228515625\n",
      "epoch 375: val avg loss is 596.4973754882812\n",
      "epoch 376: total loss is 596.4973754882812, avg loss is 596.4973754882812\n",
      "epoch 376: val avg loss is 598.22705078125\n",
      "epoch 377: total loss is 598.22705078125, avg loss is 598.22705078125\n",
      "epoch 377: val avg loss is 587.8788452148438\n",
      "epoch 378: total loss is 587.8788452148438, avg loss is 587.8788452148438\n",
      "epoch 378: val avg loss is 556.6251220703125\n",
      "epoch 379: total loss is 556.6251220703125, avg loss is 556.6251220703125\n",
      "epoch 379: val avg loss is 553.18359375\n",
      "epoch 380: total loss is 553.18359375, avg loss is 553.18359375\n",
      "epoch 380: val avg loss is 543.030517578125\n",
      "epoch 381: total loss is 543.030517578125, avg loss is 543.030517578125\n",
      "epoch 381: val avg loss is 523.6480102539062\n",
      "epoch 382: total loss is 523.6480102539062, avg loss is 523.6480102539062\n",
      "epoch 382: val avg loss is 524.5586547851562\n",
      "epoch 383: total loss is 524.5586547851562, avg loss is 524.5586547851562\n",
      "epoch 383: val avg loss is 518.3532104492188\n",
      "epoch 384: total loss is 518.3532104492188, avg loss is 518.3532104492188\n",
      "epoch 384: val avg loss is 518.305419921875\n",
      "epoch 385: total loss is 518.305419921875, avg loss is 518.305419921875\n",
      "epoch 385: val avg loss is 502.89117431640625\n",
      "epoch 386: total loss is 502.89117431640625, avg loss is 502.89117431640625\n",
      "epoch 386: val avg loss is 499.30120849609375\n",
      "epoch 387: total loss is 499.30120849609375, avg loss is 499.30120849609375\n",
      "epoch 387: val avg loss is 494.5676574707031\n",
      "epoch 388: total loss is 494.5676574707031, avg loss is 494.5676574707031\n",
      "epoch 388: val avg loss is 481.870849609375\n",
      "epoch 389: total loss is 481.870849609375, avg loss is 481.870849609375\n",
      "epoch 389: val avg loss is 473.5836181640625\n",
      "epoch 390: total loss is 473.5836181640625, avg loss is 473.5836181640625\n",
      "epoch 390: val avg loss is 464.94927978515625\n",
      "epoch 391: total loss is 464.94927978515625, avg loss is 464.94927978515625\n",
      "epoch 391: val avg loss is 462.4999084472656\n",
      "epoch 392: total loss is 462.4999084472656, avg loss is 462.4999084472656\n",
      "epoch 392: val avg loss is 457.8730773925781\n",
      "epoch 393: total loss is 457.8730773925781, avg loss is 457.8730773925781\n",
      "epoch 393: val avg loss is 452.7795104980469\n",
      "epoch 394: total loss is 452.7795104980469, avg loss is 452.7795104980469\n",
      "epoch 394: val avg loss is 448.1949157714844\n",
      "epoch 395: total loss is 448.1949157714844, avg loss is 448.1949157714844\n",
      "epoch 395: val avg loss is 442.3860168457031\n",
      "epoch 396: total loss is 442.3860168457031, avg loss is 442.3860168457031\n",
      "epoch 396: val avg loss is 435.99884033203125\n",
      "epoch 397: total loss is 435.99884033203125, avg loss is 435.99884033203125\n",
      "epoch 397: val avg loss is 432.96466064453125\n",
      "epoch 398: total loss is 432.96466064453125, avg loss is 432.96466064453125\n",
      "epoch 398: val avg loss is 427.26678466796875\n",
      "epoch 399: total loss is 427.26678466796875, avg loss is 427.26678466796875\n",
      "epoch 399: val avg loss is 423.8246765136719\n",
      "epoch 400: total loss is 423.8246765136719, avg loss is 423.8246765136719\n",
      "epoch 400: val avg loss is 419.64202880859375\n",
      "epoch 401: total loss is 419.64202880859375, avg loss is 419.64202880859375\n",
      "epoch 401: val avg loss is 414.9962158203125\n",
      "epoch 402: total loss is 414.9962158203125, avg loss is 414.9962158203125\n",
      "epoch 402: val avg loss is 410.7850036621094\n",
      "epoch 403: total loss is 410.7850036621094, avg loss is 410.7850036621094\n",
      "epoch 403: val avg loss is 406.3148193359375\n",
      "epoch 404: total loss is 406.3148193359375, avg loss is 406.3148193359375\n",
      "epoch 404: val avg loss is 402.5816650390625\n",
      "epoch 405: total loss is 402.5816650390625, avg loss is 402.5816650390625\n",
      "epoch 405: val avg loss is 399.37945556640625\n",
      "epoch 406: total loss is 399.37945556640625, avg loss is 399.37945556640625\n",
      "epoch 406: val avg loss is 396.2105712890625\n",
      "epoch 407: total loss is 396.2105712890625, avg loss is 396.2105712890625\n",
      "epoch 407: val avg loss is 392.2974853515625\n",
      "epoch 408: total loss is 392.2974853515625, avg loss is 392.2974853515625\n",
      "epoch 408: val avg loss is 389.3694763183594\n",
      "epoch 409: total loss is 389.3694763183594, avg loss is 389.3694763183594\n",
      "epoch 409: val avg loss is 386.91583251953125\n",
      "epoch 410: total loss is 386.91583251953125, avg loss is 386.91583251953125\n",
      "epoch 410: val avg loss is 385.0309753417969\n",
      "epoch 411: total loss is 385.0309753417969, avg loss is 385.0309753417969\n",
      "epoch 411: val avg loss is 384.1011962890625\n",
      "epoch 412: total loss is 384.1011962890625, avg loss is 384.1011962890625\n",
      "epoch 412: val avg loss is 382.34527587890625\n",
      "epoch 413: total loss is 382.34527587890625, avg loss is 382.34527587890625\n",
      "epoch 413: val avg loss is 379.9893798828125\n",
      "epoch 414: total loss is 379.9893798828125, avg loss is 379.9893798828125\n",
      "epoch 414: val avg loss is 377.319580078125\n",
      "epoch 415: total loss is 377.319580078125, avg loss is 377.319580078125\n",
      "epoch 415: val avg loss is 376.6617431640625\n",
      "epoch 416: total loss is 376.6617431640625, avg loss is 376.6617431640625\n",
      "epoch 416: val avg loss is 380.47955322265625\n",
      "epoch 417: total loss is 380.47955322265625, avg loss is 380.47955322265625\n",
      "epoch 417: val avg loss is 390.0188293457031\n",
      "epoch 418: total loss is 390.0188293457031, avg loss is 390.0188293457031\n",
      "epoch 418: val avg loss is 401.2658386230469\n",
      "epoch 419: total loss is 401.2658386230469, avg loss is 401.2658386230469\n",
      "epoch 419: val avg loss is 407.4693908691406\n",
      "epoch 420: total loss is 407.4693908691406, avg loss is 407.4693908691406\n",
      "epoch 420: val avg loss is 405.2829895019531\n",
      "epoch 421: total loss is 405.2829895019531, avg loss is 405.2829895019531\n",
      "epoch 421: val avg loss is 399.08843994140625\n",
      "epoch 422: total loss is 399.08843994140625, avg loss is 399.08843994140625\n",
      "epoch 422: val avg loss is 390.1790466308594\n",
      "epoch 423: total loss is 390.1790466308594, avg loss is 390.1790466308594\n",
      "epoch 423: val avg loss is 377.8949890136719\n",
      "epoch 424: total loss is 377.8949890136719, avg loss is 377.8949890136719\n",
      "epoch 424: val avg loss is 375.0271301269531\n",
      "epoch 425: total loss is 375.0271301269531, avg loss is 375.0271301269531\n",
      "epoch 425: val avg loss is 374.5543212890625\n",
      "epoch 426: total loss is 374.5543212890625, avg loss is 374.5543212890625\n",
      "epoch 426: val avg loss is 353.7819519042969\n",
      "epoch 427: total loss is 353.7819519042969, avg loss is 353.7819519042969\n",
      "epoch 427: val avg loss is 347.5959167480469\n",
      "epoch 428: total loss is 347.5959167480469, avg loss is 347.5959167480469\n",
      "epoch 428: val avg loss is 348.7715759277344\n",
      "epoch 429: total loss is 348.7715759277344, avg loss is 348.7715759277344\n",
      "epoch 429: val avg loss is 338.42572021484375\n",
      "epoch 430: total loss is 338.42572021484375, avg loss is 338.42572021484375\n",
      "epoch 430: val avg loss is 331.44122314453125\n",
      "epoch 431: total loss is 331.44122314453125, avg loss is 331.44122314453125\n",
      "epoch 431: val avg loss is 323.74169921875\n",
      "epoch 432: total loss is 323.74169921875, avg loss is 323.74169921875\n",
      "epoch 432: val avg loss is 325.0796203613281\n",
      "epoch 433: total loss is 325.0796203613281, avg loss is 325.0796203613281\n",
      "epoch 433: val avg loss is 317.39605712890625\n",
      "epoch 434: total loss is 317.39605712890625, avg loss is 317.39605712890625\n",
      "epoch 434: val avg loss is 312.5578308105469\n",
      "epoch 435: total loss is 312.5578308105469, avg loss is 312.5578308105469\n",
      "epoch 435: val avg loss is 312.2000427246094\n",
      "epoch 436: total loss is 312.2000427246094, avg loss is 312.2000427246094\n",
      "epoch 436: val avg loss is 305.6168212890625\n",
      "epoch 437: total loss is 305.6168212890625, avg loss is 305.6168212890625\n",
      "epoch 437: val avg loss is 302.59332275390625\n",
      "epoch 438: total loss is 302.59332275390625, avg loss is 302.59332275390625\n",
      "epoch 438: val avg loss is 299.7532043457031\n",
      "epoch 439: total loss is 299.7532043457031, avg loss is 299.7532043457031\n",
      "epoch 439: val avg loss is 297.36663818359375\n",
      "epoch 440: total loss is 297.36663818359375, avg loss is 297.36663818359375\n",
      "epoch 440: val avg loss is 293.2677307128906\n",
      "epoch 441: total loss is 293.2677307128906, avg loss is 293.2677307128906\n",
      "epoch 441: val avg loss is 289.81292724609375\n",
      "epoch 442: total loss is 289.81292724609375, avg loss is 289.81292724609375\n",
      "epoch 442: val avg loss is 288.0426940917969\n",
      "epoch 443: total loss is 288.0426940917969, avg loss is 288.0426940917969\n",
      "epoch 443: val avg loss is 285.513916015625\n",
      "epoch 444: total loss is 285.513916015625, avg loss is 285.513916015625\n",
      "epoch 444: val avg loss is 282.1170654296875\n",
      "epoch 445: total loss is 282.1170654296875, avg loss is 282.1170654296875\n",
      "epoch 445: val avg loss is 278.6409912109375\n",
      "epoch 446: total loss is 278.6409912109375, avg loss is 278.6409912109375\n",
      "epoch 446: val avg loss is 277.388916015625\n",
      "epoch 447: total loss is 277.388916015625, avg loss is 277.388916015625\n",
      "epoch 447: val avg loss is 274.2200927734375\n",
      "epoch 448: total loss is 274.2200927734375, avg loss is 274.2200927734375\n",
      "epoch 448: val avg loss is 271.52392578125\n",
      "epoch 449: total loss is 271.52392578125, avg loss is 271.52392578125\n",
      "epoch 449: val avg loss is 269.2934875488281\n",
      "epoch 450: total loss is 269.2934875488281, avg loss is 269.2934875488281\n",
      "epoch 450: val avg loss is 266.5859680175781\n",
      "epoch 451: total loss is 266.5859680175781, avg loss is 266.5859680175781\n",
      "epoch 451: val avg loss is 264.4881591796875\n",
      "epoch 452: total loss is 264.4881591796875, avg loss is 264.4881591796875\n",
      "epoch 452: val avg loss is 261.7760009765625\n",
      "epoch 453: total loss is 261.7760009765625, avg loss is 261.7760009765625\n",
      "epoch 453: val avg loss is 259.6839904785156\n",
      "epoch 454: total loss is 259.6839904785156, avg loss is 259.6839904785156\n",
      "epoch 454: val avg loss is 257.42523193359375\n",
      "epoch 455: total loss is 257.42523193359375, avg loss is 257.42523193359375\n",
      "epoch 455: val avg loss is 255.08119201660156\n",
      "epoch 456: total loss is 255.08119201660156, avg loss is 255.08119201660156\n",
      "epoch 456: val avg loss is 253.03936767578125\n",
      "epoch 457: total loss is 253.03936767578125, avg loss is 253.03936767578125\n",
      "epoch 457: val avg loss is 250.9609375\n",
      "epoch 458: total loss is 250.9609375, avg loss is 250.9609375\n",
      "epoch 458: val avg loss is 248.83810424804688\n",
      "epoch 459: total loss is 248.83810424804688, avg loss is 248.83810424804688\n",
      "epoch 459: val avg loss is 246.75979614257812\n",
      "epoch 460: total loss is 246.75979614257812, avg loss is 246.75979614257812\n",
      "epoch 460: val avg loss is 244.78273010253906\n",
      "epoch 461: total loss is 244.78273010253906, avg loss is 244.78273010253906\n",
      "epoch 461: val avg loss is 242.83248901367188\n",
      "epoch 462: total loss is 242.83248901367188, avg loss is 242.83248901367188\n",
      "epoch 462: val avg loss is 241.02685546875\n",
      "epoch 463: total loss is 241.02685546875, avg loss is 241.02685546875\n",
      "epoch 463: val avg loss is 239.22698974609375\n",
      "epoch 464: total loss is 239.22698974609375, avg loss is 239.22698974609375\n",
      "epoch 464: val avg loss is 237.71905517578125\n",
      "epoch 465: total loss is 237.71905517578125, avg loss is 237.71905517578125\n",
      "epoch 465: val avg loss is 236.3651580810547\n",
      "epoch 466: total loss is 236.3651580810547, avg loss is 236.3651580810547\n",
      "epoch 466: val avg loss is 235.5064697265625\n",
      "epoch 467: total loss is 235.5064697265625, avg loss is 235.5064697265625\n",
      "epoch 467: val avg loss is 235.3934326171875\n",
      "epoch 468: total loss is 235.3934326171875, avg loss is 235.3934326171875\n",
      "epoch 468: val avg loss is 236.72010803222656\n",
      "epoch 469: total loss is 236.72010803222656, avg loss is 236.72010803222656\n",
      "epoch 469: val avg loss is 240.10806274414062\n",
      "epoch 470: total loss is 240.10806274414062, avg loss is 240.10806274414062\n",
      "epoch 470: val avg loss is 246.76231384277344\n",
      "epoch 471: total loss is 246.76231384277344, avg loss is 246.76231384277344\n",
      "epoch 471: val avg loss is 256.8367614746094\n",
      "epoch 472: total loss is 256.8367614746094, avg loss is 256.8367614746094\n",
      "epoch 472: val avg loss is 268.2920837402344\n",
      "epoch 473: total loss is 268.2920837402344, avg loss is 268.2920837402344\n",
      "epoch 473: val avg loss is 272.7842712402344\n",
      "epoch 474: total loss is 272.7842712402344, avg loss is 272.7842712402344\n",
      "epoch 474: val avg loss is 264.490478515625\n",
      "epoch 475: total loss is 264.490478515625, avg loss is 264.490478515625\n",
      "epoch 475: val avg loss is 251.23733520507812\n",
      "epoch 476: total loss is 251.23733520507812, avg loss is 251.23733520507812\n",
      "epoch 476: val avg loss is 252.572265625\n",
      "epoch 477: total loss is 252.572265625, avg loss is 252.572265625\n",
      "epoch 477: val avg loss is 263.03759765625\n",
      "epoch 478: total loss is 263.03759765625, avg loss is 263.03759765625\n",
      "epoch 478: val avg loss is 257.96478271484375\n",
      "epoch 479: total loss is 257.96478271484375, avg loss is 257.96478271484375\n",
      "epoch 479: val avg loss is 242.4568634033203\n",
      "epoch 480: total loss is 242.4568634033203, avg loss is 242.4568634033203\n",
      "epoch 480: val avg loss is 242.79022216796875\n",
      "epoch 481: total loss is 242.79022216796875, avg loss is 242.79022216796875\n",
      "epoch 481: val avg loss is 246.11160278320312\n",
      "epoch 482: total loss is 246.11160278320312, avg loss is 246.11160278320312\n",
      "epoch 482: val avg loss is 232.24403381347656\n",
      "epoch 483: total loss is 232.24403381347656, avg loss is 232.24403381347656\n",
      "epoch 483: val avg loss is 227.4700469970703\n",
      "epoch 484: total loss is 227.4700469970703, avg loss is 227.4700469970703\n",
      "epoch 484: val avg loss is 239.7283935546875\n",
      "epoch 485: total loss is 239.7283935546875, avg loss is 239.7283935546875\n",
      "epoch 485: val avg loss is 250.14614868164062\n",
      "epoch 486: total loss is 250.14614868164062, avg loss is 250.14614868164062\n",
      "epoch 486: val avg loss is 266.1875915527344\n",
      "epoch 487: total loss is 266.1875915527344, avg loss is 266.1875915527344\n",
      "epoch 487: val avg loss is 276.8109130859375\n",
      "epoch 488: total loss is 276.8109130859375, avg loss is 276.8109130859375\n",
      "epoch 488: val avg loss is 264.6419982910156\n",
      "epoch 489: total loss is 264.6419982910156, avg loss is 264.6419982910156\n",
      "epoch 489: val avg loss is 236.50405883789062\n",
      "epoch 490: total loss is 236.50405883789062, avg loss is 236.50405883789062\n",
      "epoch 490: val avg loss is 216.1090087890625\n",
      "epoch 491: total loss is 216.1090087890625, avg loss is 216.1090087890625\n",
      "epoch 491: val avg loss is 219.69065856933594\n",
      "epoch 492: total loss is 219.69065856933594, avg loss is 219.69065856933594\n",
      "epoch 492: val avg loss is 220.00096130371094\n",
      "epoch 493: total loss is 220.00096130371094, avg loss is 220.00096130371094\n",
      "epoch 493: val avg loss is 211.24273681640625\n",
      "epoch 494: total loss is 211.24273681640625, avg loss is 211.24273681640625\n",
      "epoch 494: val avg loss is 212.9322509765625\n",
      "epoch 495: total loss is 212.9322509765625, avg loss is 212.9322509765625\n",
      "epoch 495: val avg loss is 206.385986328125\n",
      "epoch 496: total loss is 206.385986328125, avg loss is 206.385986328125\n",
      "epoch 496: val avg loss is 203.01589965820312\n",
      "epoch 497: total loss is 203.01589965820312, avg loss is 203.01589965820312\n",
      "epoch 497: val avg loss is 205.07398986816406\n",
      "epoch 498: total loss is 205.07398986816406, avg loss is 205.07398986816406\n",
      "epoch 498: val avg loss is 197.93994140625\n",
      "epoch 499: total loss is 197.93994140625, avg loss is 197.93994140625\n",
      "epoch 499: val avg loss is 198.3818359375\n",
      "[14044.435546875, 13878.0263671875, 13786.6708984375, 13707.44140625, 13620.70703125, 13564.42578125, 13498.1572265625, 13405.4990234375, 13331.5419921875, 13259.07421875, 13171.85546875, 13097.638671875, 13016.8876953125, 12918.9970703125, 12834.4130859375, 12738.0986328125, 12646.93359375, 12561.962890625, 12477.5849609375, 12376.453125, 12265.810546875, 12180.9931640625, 12079.86328125, 11992.1396484375, 11922.3369140625, 11867.5673828125, 11794.4228515625, 11663.6005859375, 11544.1669921875, 11477.03125, 11366.0419921875, 11282.2841796875, 11199.7392578125, 11105.400390625, 11026.7861328125, 10936.951171875, 10853.0234375, 10787.70703125, 10744.2939453125, 10683.9677734375, 10553.712890625, 10488.46484375, 10360.0791015625, 10300.0634765625, 10204.4794921875, 10140.9794921875, 10055.501953125, 10000.833984375, 9964.3203125, 9911.205078125, 9874.2607421875, 9790.3466796875, 9666.2509765625, 9588.349609375, 9483.1416015625, 9425.30859375, 9327.7470703125, 9266.908203125, 9187.58203125, 9119.41015625, 9060.001953125, 9000.81640625, 8934.83203125, 8838.958984375, 8766.3828125, 8707.986328125, 8642.6962890625, 8575.1796875, 8512.7392578125, 8439.48046875, 8366.05078125, 8301.412109375, 8254.544921875, 8221.0771484375, 8215.3828125, 8177.1748046875, 8069.2255859375, 8025.974609375, 7940.9091796875, 7832.01171875, 7836.9150390625, 7747.25732421875, 7675.7724609375, 7605.35888671875, 7527.318359375, 7475.60546875, 7416.06640625, 7354.85302734375, 7294.86962890625, 7237.9990234375, 7176.28564453125, 7127.408203125, 7065.0234375, 7012.533203125, 6954.349609375, 6902.8828125, 6852.79931640625, 6806.70556640625, 6776.2431640625, 6750.794921875, 6728.640625, 6667.3740234375, 6596.5732421875, 6538.04248046875, 6448.9462890625, 6393.68701171875, 6366.19580078125, 6284.2119140625, 6242.21875, 6185.818359375, 6137.53857421875, 6085.8037109375, 6034.734375, 5981.68212890625, 5938.33349609375, 5887.17626953125, 5841.76025390625, 5798.75537109375, 5760.54150390625, 5723.0849609375, 5682.869140625, 5633.52392578125, 5575.912109375, 5537.072265625, 5518.287109375, 5485.38134765625, 5458.9052734375, 5454.98095703125, 5444.0087890625, 5412.5634765625, 5317.2890625, 5249.2373046875, 5224.76220703125, 5185.22998046875, 5189.5732421875, 5201.14404296875, 5083.912109375, 4981.021484375, 4971.53515625, 4891.76025390625, 4862.5341796875, 4795.3544921875, 4769.3017578125, 4714.541015625, 4678.59912109375, 4637.3095703125, 4589.66259765625, 4553.474609375, 4514.7177734375, 4471.857421875, 4434.3837890625, 4396.76513671875, 4356.53759765625, 4320.66552734375, 4284.19140625, 4246.87744140625, 4209.71484375, 4174.85888671875, 4138.89599609375, 4104.58203125, 4069.419677734375, 4038.35009765625, 4009.36865234375, 3988.923583984375, 3977.74658203125, 3960.623291015625, 3910.05224609375, 3856.50537109375, 3857.7216796875, 3843.41357421875, 3812.231689453125, 3851.91064453125, 3921.236083984375, 3847.171875, 3663.831298828125, 3686.99609375, 3625.37841796875, 3597.5, 3550.04296875, 3498.92431640625, 3471.07861328125, 3437.779052734375, 3402.3623046875, 3369.7529296875, 3336.712158203125, 3310.052734375, 3279.0810546875, 3245.376953125, 3218.546875, 3190.1640625, 3157.8984375, 3128.98828125, 3098.034912109375, 3065.997314453125, 3040.855712890625, 3013.05078125, 2987.26025390625, 2959.04541015625, 2931.42431640625, 2904.054931640625, 2879.023193359375, 2854.534912109375, 2830.693359375, 2810.199951171875, 2792.546630859375, 2782.866943359375, 2783.110107421875, 2780.02001953125, 2747.737548828125, 2680.891845703125, 2638.4228515625, 2632.882080078125, 2611.7685546875, 2584.603271484375, 2573.5126953125, 2570.999267578125, 2568.677734375, 2536.090087890625, 2476.4130859375, 2449.782958984375, 2444.6064453125, 2414.17041015625, 2397.708740234375, 2385.097412109375, 2387.6552734375, 2418.24755859375, 2425.771240234375, 2361.968017578125, 2291.60595703125, 2277.802001953125, 2273.94091796875, 2249.992431640625, 2192.52734375, 2172.65283203125, 2157.90087890625, 2116.055908203125, 2107.64892578125, 2081.7509765625, 2049.502685546875, 2037.165771484375, 2011.2965087890625, 1996.2222900390625, 1972.004638671875, 1956.0267333984375, 1932.455322265625, 1916.6409912109375, 1894.7432861328125, 1878.6566162109375, 1859.5843505859375, 1840.781982421875, 1824.86572265625, 1807.213623046875, 1790.8016357421875, 1772.8441162109375, 1758.3294677734375, 1742.0767822265625, 1726.785888671875, 1711.4896240234375, 1696.841064453125, 1680.5982666015625, 1662.9281005859375, 1648.03564453125, 1638.5738525390625, 1638.85595703125, 1648.9664306640625, 1665.119384765625, 1679.038818359375, 1687.618408203125, 1672.819091796875, 1624.1064453125, 1612.3099365234375, 1649.8841552734375, 1627.4200439453125, 1582.3870849609375, 1552.9063720703125, 1511.334228515625, 1477.9217529296875, 1458.1270751953125, 1434.781494140625, 1425.3375244140625, 1396.34375, 1384.465087890625, 1366.895263671875, 1347.99658203125, 1339.0244140625, 1316.5894775390625, 1306.105224609375, 1290.1715087890625, 1277.1297607421875, 1264.9715576171875, 1246.94482421875, 1237.322021484375, 1224.0792236328125, 1210.3182373046875, 1198.628173828125, 1185.3104248046875, 1174.493896484375, 1161.740234375, 1150.6766357421875, 1140.06982421875, 1131.285400390625, 1124.375, 1123.9049072265625, 1134.697998046875, 1160.958984375, 1186.9075927734375, 1162.0889892578125, 1084.2537841796875, 1056.3062744140625, 1068.54052734375, 1045.5882568359375, 1029.7576904296875, 1017.22998046875, 1004.3388671875, 995.7150268554688, 986.1122436523438, 968.3827514648438, 960.695068359375, 951.5878295898438, 938.0834350585938, 929.9832153320312, 918.4680786132812, 911.510498046875, 899.8463134765625, 889.5936889648438, 882.0819702148438, 871.8624267578125, 863.1610717773438, 854.0006713867188, 846.3924560546875, 836.30419921875, 827.8927612304688, 820.0770874023438, 811.2142333984375, 802.9965209960938, 795.23193359375, 787.0164794921875, 779.058837890625, 771.3414916992188, 764.0221557617188, 756.501953125, 749.876220703125, 744.5530395507812, 741.2354125976562, 742.333740234375, 748.832275390625, 757.1585693359375, 749.5521240234375, 721.3392944335938, 713.0347900390625, 732.28271484375, 729.6954956054688, 717.1173706054688, 692.3123779296875, 667.3677368164062, 672.6405029296875, 658.8734741210938, 650.7106323242188, 641.973388671875, 634.006591796875, 628.9782104492188, 620.5965576171875, 616.2100219726562, 607.2001342773438, 605.2723999023438, 599.3519287109375, 600.2591552734375, 602.0992431640625, 615.5095825195312, 631.7899169921875, 645.06787109375, 630.6419677734375, 593.639404296875, 579.8595581054688, 602.7868041992188, 608.8834228515625, 596.4973754882812, 598.22705078125, 587.8788452148438, 556.6251220703125, 553.18359375, 543.030517578125, 523.6480102539062, 524.5586547851562, 518.3532104492188, 518.305419921875, 502.89117431640625, 499.30120849609375, 494.5676574707031, 481.870849609375, 473.5836181640625, 464.94927978515625, 462.4999084472656, 457.8730773925781, 452.7795104980469, 448.1949157714844, 442.3860168457031, 435.99884033203125, 432.96466064453125, 427.26678466796875, 423.8246765136719, 419.64202880859375, 414.9962158203125, 410.7850036621094, 406.3148193359375, 402.5816650390625, 399.37945556640625, 396.2105712890625, 392.2974853515625, 389.3694763183594, 386.91583251953125, 385.0309753417969, 384.1011962890625, 382.34527587890625, 379.9893798828125, 377.319580078125, 376.6617431640625, 380.47955322265625, 390.0188293457031, 401.2658386230469, 407.4693908691406, 405.2829895019531, 399.08843994140625, 390.1790466308594, 377.8949890136719, 375.0271301269531, 374.5543212890625, 353.7819519042969, 347.5959167480469, 348.7715759277344, 338.42572021484375, 331.44122314453125, 323.74169921875, 325.0796203613281, 317.39605712890625, 312.5578308105469, 312.2000427246094, 305.6168212890625, 302.59332275390625, 299.7532043457031, 297.36663818359375, 293.2677307128906, 289.81292724609375, 288.0426940917969, 285.513916015625, 282.1170654296875, 278.6409912109375, 277.388916015625, 274.2200927734375, 271.52392578125, 269.2934875488281, 266.5859680175781, 264.4881591796875, 261.7760009765625, 259.6839904785156, 257.42523193359375, 255.08119201660156, 253.03936767578125, 250.9609375, 248.83810424804688, 246.75979614257812, 244.78273010253906, 242.83248901367188, 241.02685546875, 239.22698974609375, 237.71905517578125, 236.3651580810547, 235.5064697265625, 235.3934326171875, 236.72010803222656, 240.10806274414062, 246.76231384277344, 256.8367614746094, 268.2920837402344, 272.7842712402344, 264.490478515625, 251.23733520507812, 252.572265625, 263.03759765625, 257.96478271484375, 242.4568634033203, 242.79022216796875, 246.11160278320312, 232.24403381347656, 227.4700469970703, 239.7283935546875, 250.14614868164062, 266.1875915527344, 276.8109130859375, 264.6419982910156, 236.50405883789062, 216.1090087890625, 219.69065856933594, 220.00096130371094, 211.24273681640625, 212.9322509765625, 206.385986328125, 203.01589965820312, 205.07398986816406, 197.93994140625] [13878.0263671875, 13786.6708984375, 13707.44140625, 13620.70703125, 13564.42578125, 13498.1572265625, 13405.4990234375, 13331.5419921875, 13259.07421875, 13171.85546875, 13097.638671875, 13016.8876953125, 12918.9970703125, 12834.4130859375, 12738.0986328125, 12646.93359375, 12561.962890625, 12477.5849609375, 12376.453125, 12265.810546875, 12180.9931640625, 12079.86328125, 11992.1396484375, 11922.3369140625, 11867.5673828125, 11794.4228515625, 11663.6005859375, 11544.1669921875, 11477.03125, 11366.0419921875, 11282.2841796875, 11199.7392578125, 11105.400390625, 11026.7861328125, 10936.951171875, 10853.0234375, 10787.70703125, 10744.2939453125, 10683.9677734375, 10553.712890625, 10488.46484375, 10360.0791015625, 10300.0634765625, 10204.4794921875, 10140.9794921875, 10055.501953125, 10000.833984375, 9964.3203125, 9911.205078125, 9874.2607421875, 9790.3466796875, 9666.2509765625, 9588.349609375, 9483.1416015625, 9425.30859375, 9327.7470703125, 9266.908203125, 9187.58203125, 9119.41015625, 9060.001953125, 9000.81640625, 8934.83203125, 8838.958984375, 8766.3828125, 8707.986328125, 8642.6962890625, 8575.1796875, 8512.7392578125, 8439.48046875, 8366.05078125, 8301.412109375, 8254.544921875, 8221.0771484375, 8215.3828125, 8177.1748046875, 8069.2255859375, 8025.974609375, 7940.9091796875, 7832.01171875, 7836.9150390625, 7747.25732421875, 7675.7724609375, 7605.35888671875, 7527.318359375, 7475.60546875, 7416.06640625, 7354.85302734375, 7294.86962890625, 7237.9990234375, 7176.28564453125, 7127.408203125, 7065.0234375, 7012.533203125, 6954.349609375, 6902.8828125, 6852.79931640625, 6806.70556640625, 6776.2431640625, 6750.794921875, 6728.640625, 6667.3740234375, 6596.5732421875, 6538.04248046875, 6448.9462890625, 6393.68701171875, 6366.19580078125, 6284.2119140625, 6242.21875, 6185.818359375, 6137.53857421875, 6085.8037109375, 6034.734375, 5981.68212890625, 5938.33349609375, 5887.17626953125, 5841.76025390625, 5798.75537109375, 5760.54150390625, 5723.0849609375, 5682.869140625, 5633.52392578125, 5575.912109375, 5537.072265625, 5518.287109375, 5485.38134765625, 5458.9052734375, 5454.98095703125, 5444.0087890625, 5412.5634765625, 5317.2890625, 5249.2373046875, 5224.76220703125, 5185.22998046875, 5189.5732421875, 5201.14404296875, 5083.912109375, 4981.021484375, 4971.53515625, 4891.76025390625, 4862.5341796875, 4795.3544921875, 4769.3017578125, 4714.541015625, 4678.59912109375, 4637.3095703125, 4589.66259765625, 4553.474609375, 4514.7177734375, 4471.857421875, 4434.3837890625, 4396.76513671875, 4356.53759765625, 4320.66552734375, 4284.19140625, 4246.87744140625, 4209.71484375, 4174.85888671875, 4138.89599609375, 4104.58203125, 4069.419677734375, 4038.35009765625, 4009.36865234375, 3988.923583984375, 3977.74658203125, 3960.623291015625, 3910.05224609375, 3856.50537109375, 3857.7216796875, 3843.41357421875, 3812.231689453125, 3851.91064453125, 3921.236083984375, 3847.171875, 3663.831298828125, 3686.99609375, 3625.37841796875, 3597.5, 3550.04296875, 3498.92431640625, 3471.07861328125, 3437.779052734375, 3402.3623046875, 3369.7529296875, 3336.712158203125, 3310.052734375, 3279.0810546875, 3245.376953125, 3218.546875, 3190.1640625, 3157.8984375, 3128.98828125, 3098.034912109375, 3065.997314453125, 3040.855712890625, 3013.05078125, 2987.26025390625, 2959.04541015625, 2931.42431640625, 2904.054931640625, 2879.023193359375, 2854.534912109375, 2830.693359375, 2810.199951171875, 2792.546630859375, 2782.866943359375, 2783.110107421875, 2780.02001953125, 2747.737548828125, 2680.891845703125, 2638.4228515625, 2632.882080078125, 2611.7685546875, 2584.603271484375, 2573.5126953125, 2570.999267578125, 2568.677734375, 2536.090087890625, 2476.4130859375, 2449.782958984375, 2444.6064453125, 2414.17041015625, 2397.708740234375, 2385.097412109375, 2387.6552734375, 2418.24755859375, 2425.771240234375, 2361.968017578125, 2291.60595703125, 2277.802001953125, 2273.94091796875, 2249.992431640625, 2192.52734375, 2172.65283203125, 2157.90087890625, 2116.055908203125, 2107.64892578125, 2081.7509765625, 2049.502685546875, 2037.165771484375, 2011.2965087890625, 1996.2222900390625, 1972.004638671875, 1956.0267333984375, 1932.455322265625, 1916.6409912109375, 1894.7432861328125, 1878.6566162109375, 1859.5843505859375, 1840.781982421875, 1824.86572265625, 1807.213623046875, 1790.8016357421875, 1772.8441162109375, 1758.3294677734375, 1742.0767822265625, 1726.785888671875, 1711.4896240234375, 1696.841064453125, 1680.5982666015625, 1662.9281005859375, 1648.03564453125, 1638.5738525390625, 1638.85595703125, 1648.9664306640625, 1665.119384765625, 1679.038818359375, 1687.618408203125, 1672.819091796875, 1624.1064453125, 1612.3099365234375, 1649.8841552734375, 1627.4200439453125, 1582.3870849609375, 1552.9063720703125, 1511.334228515625, 1477.9217529296875, 1458.1270751953125, 1434.781494140625, 1425.3375244140625, 1396.34375, 1384.465087890625, 1366.895263671875, 1347.99658203125, 1339.0244140625, 1316.5894775390625, 1306.105224609375, 1290.1715087890625, 1277.1297607421875, 1264.9715576171875, 1246.94482421875, 1237.322021484375, 1224.0792236328125, 1210.3182373046875, 1198.628173828125, 1185.3104248046875, 1174.493896484375, 1161.740234375, 1150.6766357421875, 1140.06982421875, 1131.285400390625, 1124.375, 1123.9049072265625, 1134.697998046875, 1160.958984375, 1186.9075927734375, 1162.0889892578125, 1084.2537841796875, 1056.3062744140625, 1068.54052734375, 1045.5882568359375, 1029.7576904296875, 1017.22998046875, 1004.3388671875, 995.7150268554688, 986.1122436523438, 968.3827514648438, 960.695068359375, 951.5878295898438, 938.0834350585938, 929.9832153320312, 918.4680786132812, 911.510498046875, 899.8463134765625, 889.5936889648438, 882.0819702148438, 871.8624267578125, 863.1610717773438, 854.0006713867188, 846.3924560546875, 836.30419921875, 827.8927612304688, 820.0770874023438, 811.2142333984375, 802.9965209960938, 795.23193359375, 787.0164794921875, 779.058837890625, 771.3414916992188, 764.0221557617188, 756.501953125, 749.876220703125, 744.5530395507812, 741.2354125976562, 742.333740234375, 748.832275390625, 757.1585693359375, 749.5521240234375, 721.3392944335938, 713.0347900390625, 732.28271484375, 729.6954956054688, 717.1173706054688, 692.3123779296875, 667.3677368164062, 672.6405029296875, 658.8734741210938, 650.7106323242188, 641.973388671875, 634.006591796875, 628.9782104492188, 620.5965576171875, 616.2100219726562, 607.2001342773438, 605.2723999023438, 599.3519287109375, 600.2591552734375, 602.0992431640625, 615.5095825195312, 631.7899169921875, 645.06787109375, 630.6419677734375, 593.639404296875, 579.8595581054688, 602.7868041992188, 608.8834228515625, 596.4973754882812, 598.22705078125, 587.8788452148438, 556.6251220703125, 553.18359375, 543.030517578125, 523.6480102539062, 524.5586547851562, 518.3532104492188, 518.305419921875, 502.89117431640625, 499.30120849609375, 494.5676574707031, 481.870849609375, 473.5836181640625, 464.94927978515625, 462.4999084472656, 457.8730773925781, 452.7795104980469, 448.1949157714844, 442.3860168457031, 435.99884033203125, 432.96466064453125, 427.26678466796875, 423.8246765136719, 419.64202880859375, 414.9962158203125, 410.7850036621094, 406.3148193359375, 402.5816650390625, 399.37945556640625, 396.2105712890625, 392.2974853515625, 389.3694763183594, 386.91583251953125, 385.0309753417969, 384.1011962890625, 382.34527587890625, 379.9893798828125, 377.319580078125, 376.6617431640625, 380.47955322265625, 390.0188293457031, 401.2658386230469, 407.4693908691406, 405.2829895019531, 399.08843994140625, 390.1790466308594, 377.8949890136719, 375.0271301269531, 374.5543212890625, 353.7819519042969, 347.5959167480469, 348.7715759277344, 338.42572021484375, 331.44122314453125, 323.74169921875, 325.0796203613281, 317.39605712890625, 312.5578308105469, 312.2000427246094, 305.6168212890625, 302.59332275390625, 299.7532043457031, 297.36663818359375, 293.2677307128906, 289.81292724609375, 288.0426940917969, 285.513916015625, 282.1170654296875, 278.6409912109375, 277.388916015625, 274.2200927734375, 271.52392578125, 269.2934875488281, 266.5859680175781, 264.4881591796875, 261.7760009765625, 259.6839904785156, 257.42523193359375, 255.08119201660156, 253.03936767578125, 250.9609375, 248.83810424804688, 246.75979614257812, 244.78273010253906, 242.83248901367188, 241.02685546875, 239.22698974609375, 237.71905517578125, 236.3651580810547, 235.5064697265625, 235.3934326171875, 236.72010803222656, 240.10806274414062, 246.76231384277344, 256.8367614746094, 268.2920837402344, 272.7842712402344, 264.490478515625, 251.23733520507812, 252.572265625, 263.03759765625, 257.96478271484375, 242.4568634033203, 242.79022216796875, 246.11160278320312, 232.24403381347656, 227.4700469970703, 239.7283935546875, 250.14614868164062, 266.1875915527344, 276.8109130859375, 264.6419982910156, 236.50405883789062, 216.1090087890625, 219.69065856933594, 220.00096130371094, 211.24273681640625, 212.9322509765625, 206.385986328125, 203.01589965820312, 205.07398986816406, 197.93994140625, 198.3818359375]\n"
     ]
    }
   ],
   "source": [
    "# SVDNet learning to factor a single image\n",
    "net = SVDNet(128, 128)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-1)\n",
    "\n",
    "def loss(x, x_hat):\n",
    "    return ((x - x_hat)**2).sum()\n",
    "\n",
    "experiment = Experiment(\n",
    "    net=net, loss=loss, optimizer=optimizer,\n",
    "    train_dataloader=single_ele_dataloader, validation_dataloader=single_ele_dataloader,\n",
    "    use_eye_as_net_input=True, inputs_are_ground_truth=True\n",
    ")\n",
    "\n",
    "train_loss_over_epochs, val_loss_over_epochs = experiment.run(train_epochs=500, train_validation_interval=1)\n",
    "print(train_loss_over_epochs, val_loss_over_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
